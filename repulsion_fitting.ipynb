{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "repulsion_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AWcTW2B0DEu",
        "colab_type": "text"
      },
      "source": [
        "run these code block **sequentially** use shift+Enter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiGdOtp2CmDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameter\n",
        "\n",
        "BATCH_SIZE = 8192\n",
        "EPOCHS = 1000\n",
        "train_size = 300000\n",
        "test_size =  10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29liKyfiK60k",
        "colab_type": "code",
        "outputId": "5f744e23-fcdf-4db3-ad4d-e4df91366c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from google.colab import files\n",
        "import csv\n",
        "from sklearn.preprocessing import *\n",
        "import os, re, math, json, shutil, pprint\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import IPython.display as display\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "#tf.enable_eager_execution()\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "print(\"Keras version \" + keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 1.15.0\n",
            "Keras version 2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKSmzH6EBBNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title visualization utilities [RUN ME]\n",
        "\"\"\"\n",
        "This cell contains helper functions used for visualization\n",
        "and downloads only. You can skip reading it. There is very\n",
        "little useful Keras/Tensorflow code here.\n",
        "\"\"\"\n",
        "\n",
        "# Matplotlib config\n",
        "plt.ioff()\n",
        "plt.rc('image', cmap='gray_r')\n",
        "plt.rc('grid', linewidth=1)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0', figsize=(16,9))\n",
        "# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
        "  \n",
        "  # get one batch from each: 10000 validation digits, N training digits\n",
        "  batch_train_ds = training_dataset.apply(tf.data.experimental.unbatch()).batch(N)\n",
        "  \n",
        "  # eager execution: loop through datasets normally\n",
        "  if tf.executing_eagerly():\n",
        "    for validation_digits, validation_labels in validation_dataset:\n",
        "      validation_digits = validation_digits.numpy()\n",
        "      validation_labels = validation_labels.numpy()\n",
        "      break\n",
        "    for training_digits, training_labels in batch_train_ds:\n",
        "      training_digits = training_digits.numpy()\n",
        "      training_labels = training_labels.numpy()\n",
        "      break\n",
        "    \n",
        "  else:\n",
        "    v_images, v_labels = validation_dataset.make_one_shot_iterator().get_next()\n",
        "    t_images, t_labels = batch_train_ds.make_one_shot_iterator().get_next()\n",
        "    # Run once, get one batch. Session.run returns numpy results\n",
        "    with tf.Session() as ses:\n",
        "      (validation_digits, validation_labels,\n",
        "       training_digits, training_labels) = ses.run([v_images, v_labels, t_images, t_labels])\n",
        "  \n",
        "  # these were one-hot encoded in the dataset\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\n",
        "  training_labels = np.argmax(training_labels, axis=1)\n",
        "  \n",
        "  return (training_digits, training_labels,\n",
        "          validation_digits, validation_labels)\n",
        "\n",
        "# create digits from local fonts for testing\n",
        "def create_digits_from_local_fonts(n):\n",
        "  font_labels = []\n",
        "  img = PIL.Image.new('LA', (28*n, 28), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
        "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
        "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
        "  d = PIL.ImageDraw.Draw(img)\n",
        "  for i in range(n):\n",
        "    font_labels.append(i%10)\n",
        "    d.text((7+i*28,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
        "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
        "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [28, 28*n]), n, axis=1), axis=0), [n, 28*28])\n",
        "  return font_digits, font_labels\n",
        "\n",
        "# utility to display a row of digits with their predictions\n",
        "def display_digits(digits, predictions, labels, title, n):\n",
        "  fig = plt.figure(figsize=(13,3))\n",
        "  digits = np.reshape(digits, [n, 28, 28])\n",
        "  digits = np.swapaxes(digits, 0, 1)\n",
        "  digits = np.reshape(digits, [28, 28*n])\n",
        "  plt.yticks([])\n",
        "  plt.xticks([28*x+14 for x in range(n)], predictions)\n",
        "  plt.grid(b=None)\n",
        "  for i,t in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
        "    if predictions[i] != labels[i]: t.set_color('red') # bad predictions in red\n",
        "  plt.imshow(digits)\n",
        "  plt.grid(None)\n",
        "  plt.title(title)\n",
        "  display.display(fig)\n",
        "  \n",
        "# utility to display multiple rows of digits, sorted by unrecognized/recognized status\n",
        "def display_top_unrecognized(digits, predictions, labels, n, lines):\n",
        "  idx = np.argsort(predictions==labels) # sort order: unrecognized first\n",
        "  for i in range(lines):\n",
        "    display_digits(digits[idx][i*n:(i+1)*n], predictions[idx][i*n:(i+1)*n], labels[idx][i*n:(i+1)*n],\n",
        "                   \"{} sample validation digits out of {} with bad predictions in red and sorted first\".format(n*lines, len(digits)) if i==0 else \"\", n)\n",
        "\n",
        "def plot_learning_rate(lr_func, epochs):\n",
        "  xx = np.arange(epochs+1, dtype=np.float)\n",
        "  y = [lr_decay(x) for x in xx]\n",
        "  fig, ax = plt.subplots(figsize=(9, 6))\n",
        "  ax.set_xlabel('epochs')\n",
        "  ax.set_title('Learning rate\\ndecays from {:0.3g} to {:0.3g}'.format(y[0], y[-2]))\n",
        "  ax.minorticks_on()\n",
        "  ax.grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
        "  ax.grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
        "  ax.step(xx,y, linewidth=3, where='post')\n",
        "  display.display(fig)\n",
        "\n",
        "class PlotTraining(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, sample_rate=1, zoom=1):\n",
        "    self.sample_rate = sample_rate\n",
        "    self.step = 0\n",
        "    self.zoom = zoom\n",
        "    self.steps_per_epoch = 60000//BATCH_SIZE\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.batch_history = {}\n",
        "    self.batch_step = []\n",
        "    self.epoch_history = {}\n",
        "    self.epoch_step = []\n",
        "    self.fig, self.axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "    plt.ioff()\n",
        "\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    if (batch % self.sample_rate) == 0:\n",
        "      self.batch_step.append(self.step)\n",
        "      for k,v in logs.items():\n",
        "        # do not log \"batch\" and \"size\" metrics that do not change\n",
        "        # do not log training accuracy \"acc\"\n",
        "        if k=='batch' or k=='size':# or k=='acc':\n",
        "          continue\n",
        "        self.batch_history.setdefault(k, []).append(v)\n",
        "    self.step += 1\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    plt.close(self.fig)\n",
        "    self.axes[0].cla()\n",
        "    self.axes[1].cla()\n",
        "      \n",
        "    self.axes[0].set_ylim(0, 1/self.zoom)\n",
        "    self.axes[1].set_ylim(0, 1+0.1/self.zoom/2)\n",
        "    \n",
        "    self.epoch_step.append(self.step)\n",
        "    for k,v in logs.items():\n",
        "      # only log validation metrics\n",
        "      if not k.startswith('val_'):\n",
        "        continue\n",
        "      self.epoch_history.setdefault(k, []).append(v)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    \n",
        "    for k,v in self.batch_history.items():\n",
        "      self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.batch_step) / self.steps_per_epoch, v, label=k)\n",
        "      \n",
        "    for k,v in self.epoch_history.items():\n",
        "      self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.epoch_step) / self.steps_per_epoch, v, label=k, linewidth=3)\n",
        "      \n",
        "    self.axes[0].legend()\n",
        "    self.axes[1].legend()\n",
        "    self.axes[0].set_xlabel('epochs')\n",
        "    self.axes[1].set_xlabel('epochs')\n",
        "    self.axes[0].minorticks_on()\n",
        "    self.axes[0].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
        "    self.axes[0].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
        "    self.axes[1].minorticks_on()\n",
        "    self.axes[1].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
        "    self.axes[1].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
        "    display.display(self.fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKZNOFr2mwl9",
        "colab_type": "text"
      },
      "source": [
        "the next part is to add train and test data,\n",
        "click run and upload from local environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84_KL-hEMGNG",
        "colab_type": "code",
        "outputId": "decf4013-a757-4a7e-f815-608dc6c39152",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-061f61ec-0fcd-474f-a16c-e31a03504de3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-061f61ec-0fcd-474f-a16c-e31a03504de3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving repulsion_no0.csv to repulsion_no0.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34KSRHjrNWk8",
        "colab_type": "code",
        "outputId": "b8db5ac3-2bd0-4bda-f2ea-c03a623ce932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('repulsion_no0.csv', header=None)\n",
        "\n",
        "# shuffle data if necessary\n",
        "df = df.iloc[0:400000,:]\n",
        "# df = df.sample(frac=1, axis=0).reset_index(drop=False)\n",
        "\n",
        "num = 1e14\n",
        "# # format of train/test data\n",
        "# # [x y z a/b]\n",
        "# # energy\n",
        "train_cord = df.iloc[0:train_size, 0:4].values\n",
        "train_cord = train_cord\n",
        "train_label1 = df.iloc[0:train_size, 4].values * num\n",
        "train_label1 = np.reshape(train_label1, (train_size, 1)) \n",
        "\n",
        "valid_cord = df.iloc[train_size:(train_size+test_size), 0:4].values\n",
        "valid_cord = valid_cord\n",
        "valid_label1 = df.iloc[train_size:(train_size+test_size),4].values * num\n",
        "valid_label1 = np.reshape(valid_label1, (test_size, 1))\n",
        "\n",
        "\n",
        "print(np.amin(train_label1), np.amin(valid_label1))\n",
        "print(np.amax(train_label1), np.amax(valid_label1))\n",
        "\n",
        "# count = []\n",
        "# lower = int(np.amin(train_label1))\n",
        "# upper = int(np.amax(train_label1))+1\n",
        "# for i in range(lower,upper):\n",
        "#   n = np.size(train_label1[(train_label1 < i + 0.5) & (train_label1 > i - 0.5)])\n",
        "#   count.append(n)\n",
        "# print(count)\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.plot(np.linspace(lower,upper,len(count)),np.array(count))\n",
        "\n",
        "# print(train_cord[0:10])\n",
        "# print(valid_cord[0:10])\n",
        "\n",
        "# power transformation of energy\n",
        "# from train data\n",
        "scaler = PowerTransformer(method=\"box-cox\")\n",
        "train_label2 = scaler.fit_transform(train_label1)\n",
        "valid_label2 = scaler.transform(valid_label1)\n",
        "\n",
        "train_label = train_label2\n",
        "valid_label = valid_label2\n",
        "\n",
        "# df2 = pd.DataFrame(np.vstack([train_label, valid_label]))\n",
        "# df2.to_csv('output_transformed.csv', index=False, header=False)\n",
        "# print(np.min(train_label),np.max(train_label), np.min(train_cord), np.max(train_cord), np.mean(train_label))\n",
        "\n",
        "print(np.amin(train_label), np.amin(valid_label))\n",
        "print(np.amax(train_label), np.amax(valid_label))\n",
        "# count = []\n",
        "# tl3 = train_label * 10\n",
        "# lower = int(np.amin(tl3))\n",
        "# upper = int(np.amax(tl3))+1\n",
        "# for i in range(lower,upper):\n",
        "#   n = np.size(tl3[(tl3 < i + 0.5) & (tl3 > i - 0.5)])\n",
        "#   count.append(n)\n",
        "# print(count)\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.plot(np.linspace(lower,upper,len(count)),np.array(count))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 1.0\n",
            "83691026100000.0 58423054400000.0\n",
            "-1.7569570087885478 -1.7569570087885478\n",
            "2.3932736568068047 2.3407419800347418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Laf1kWvNy01Y",
        "colab_type": "text"
      },
      "source": [
        "check train and test data dimensions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NahhIBaNo3l",
        "colab_type": "code",
        "outputId": "8bee22ac-c4ea-4213-c79f-1f6c3ddc8b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_cord.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8UBr2vb9IEJ",
        "colab_type": "code",
        "outputId": "adc7098c-23cd-42e1-ba50-76e915ad859d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_label1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEt5yJBMlRRf",
        "colab_type": "code",
        "outputId": "1a8f3d76-e87b-47c2-cefe-845dd97929ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "valid_cord.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX-KyU8clOyn",
        "colab_type": "code",
        "outputId": "e73ef04e-53fa-49b8-e5f7-7e281b696f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "valid_label1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q56OGxwczCFa",
        "colab_type": "text"
      },
      "source": [
        "this is the part to set up NN model\n",
        "sequentially we add layers with # of neurons and activation defined\n",
        "\n",
        "the shape of the layers will printed in the next section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZ1J_f5Nvln",
        "colab_type": "code",
        "outputId": "c1e393c9-e1b8-4cd7-eef6-1634278b2e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "elu = keras.layers.ELU(alpha=2.0)\n",
        "network.add(layers.Dense(40, activation='elu', input_shape=(train_cord.shape[1],)))\n",
        "network.add(layers.Dense(40, activation='elu'))\n",
        "network.add(layers.Dense(30, activation='elu'))\n",
        "network.add(layers.Dense(30, activation='elu'))\n",
        "network.add(layers.Dense(20, activation='elu'))\n",
        "network.add(layers.Dense(20, activation='elu'))\n",
        "network.add(layers.Dense(15, activation='elu'))\n",
        "network.add(layers.Dense(15, activation='elu'))\n",
        "network.add(layers.Dense(10, activation='elu'))\n",
        "network.add(layers.Dense(10, activation='elu'))\n",
        "network.add(layers.Dense(5, activation='elu'))\n",
        "network.add(layers.Dense(5, activation='elu'))\n",
        "network.add(layers.Dense(1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK1gyR0vzODZ",
        "colab_type": "text"
      },
      "source": [
        "this is the part to set up your optimizer for fitting the model\n",
        "we use Stochastic gradient descent (please refer to documentation for details)\n",
        "\n",
        "also compile the whole thing with the settings of optimizer, loss function of mean square error, and metrics of mean absolute error\n",
        "\n",
        "set up a cool learning rate feature to control step of optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q7HJyfSRYyJ",
        "colab_type": "code",
        "outputId": "11b44547-35f4-4c8c-975a-4f46172eba1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "# All parameter gradients will be clipped to\n",
        "# a maximum norm of 1.\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "network.compile(optimizer=adam,\n",
        "                loss='mse',\n",
        "                metrics=['mae'])\n",
        "\n",
        "# lr decay function\n",
        "# def lr_decay(epoch):\n",
        "  # return 1 * math.pow(0.97, epoch)\n",
        "\n",
        "# lr schedule callback\n",
        "# lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\n",
        "\n",
        "# important to see what you are doing\n",
        "# plot_learning_rate(lr_decay, EPOCHS)\n",
        "\n",
        "\n",
        "# print model layers\n",
        "network.summary()\n",
        "\n",
        "# utility callback that displays training curves\n",
        "plot_training = PlotTraining(sample_rate=10, zoom=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 40)                200       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 40)                1640      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 30)                1230      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 30)                930       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                160       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 5,956\n",
            "Trainable params: 5,956\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6k-ckZjDCvH",
        "colab_type": "code",
        "outputId": "cf957a9d-bc77-41af-9cee-13dd3e73179f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "steps_per_epoch = train_size//BATCH_SIZE \n",
        "print(\"Steps per epoch: \", steps_per_epoch)\n",
        "\n",
        "\n",
        "history = network.fit(train_cord, train_label, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                      validation_data=(valid_cord, valid_label), validation_steps=steps_per_epoch, callbacks=[plot_training])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1a2488a30762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Steps per epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = network.fit(train_cord, train_label, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1HMp5btwqdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for layer in network.layers:\n",
        "#    weights = layer.get_weights()\n",
        "#   print(*(node for node in weights))# ( for node in layer.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Tj2BOjUST-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pred = network.predict(train_cord)\n",
        "valid_pred = network.predict(valid_cord)\n",
        "\n",
        "train_pred_inverse = scaler.inverse_transform(train_pred) / num\n",
        "valid_pred_inverse = scaler.inverse_transform(valid_pred) / num\n",
        "\n",
        "# train_pred_inverse = np.exp(train_pred) / num\n",
        "# valid_pred_inverse = np.exp(valid_pred) / num\n",
        "\n",
        "# print(train_pred_inverse[0:100])\n",
        "print(df[0:10])\n",
        "print(train_pred_inverse[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOWChmh3xsib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}